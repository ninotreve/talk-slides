\documentclass[aspectratio=169]{beamer}
\usetheme{Darmstadt}

\usepackage{amsmath, amssymb}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{matrix,decorations.pathmorphing,calc,decorations.pathreplacing,topaths,fit,positioning,automata,patterns,arrows,shapes,chains}
\tikzstyle{line} = [draw, -latex']

% Commands
\newcommand{\dd}{\,\text{d}}
\newcommand{\T}{\text{T}}
\newcommand{\B}[1]{\mathbf{#1}} % shortcut for \mathbf
\newcommand{\Bs}[1]{\boldsymbol{#1}} % shortcut for \boldsymbol
\newcommand{\pr}[1]{\left(#1\right)} % shortcut for \left(\right)
\newcommand{\br}[1]{\left[#1\right]} % shortcut for \left[\right]
\newcommand{\bbr}[1]{\left\{#1\right\}} % shortcut for \left\{\right\}
\newcommand{\nr}[1]{\left\|#1\right\|} % shortcut for \left\|\right\|

\title{High-Dimensional Gaussian Sampling}
\author{Yuejia Zhang}
\date{April 7, 2023}
\begin{document}
	
\begin{frame}
    \maketitle
\end{frame}

\section{Introduction} 
\subsection{Problem}
\begin{frame}
\frametitle{Problem Definition}
Sampling from a \(d\)-dimensional Gaussian distribution \(\mathcal{N}\pr{\Bs{\mu},\B{\Sigma}}\), where \(d\) may be large.

\[
    \pi(\Bs{\theta}) = \dfrac{1}{(2\pi)^{d/2}\text{det}(\B{\Sigma})^{1/2}}\exp\pr{-\dfrac{1}{2}(\Bs{\theta}-\Bs{\mu})^{\top}\B{\Sigma}^{-1}(\Bs{\theta}-\Bs{\mu})}.
\]

Covariance matrix \(\B{\Sigma}\) positive definite. Precision matrix \(\B{Q}=\B{\Sigma}^{-1}\) exists and also positive definite.
\end{frame}

\subsection{Motivation}
\begin{frame}
\frametitle{Special Cases}
\begin{itemize}
\item \(d = 1\)
\begin{algorithm}[H]
\caption{Box--Muller sampler}
\begin{algorithmic}[1]
\State Draw $u_1$, $u_2$ $\overset{\text{i.i.d.}}{\sim} \mathcal{U}((0,1])$.
\State Set $\tilde{u}_1 = \sqrt{-2\log(u_1)}$.
\State Set $\tilde{u}_2 = 2\pi u_2$.\\ 
\Return $(\theta_1,\theta_2) = \pr{\mu + \frac{\tilde{u}_1}{\sqrt{q}} \sin(\tilde{u}_2),\mu + \frac{\tilde{u}_1}{\sqrt{q}} \cos(\tilde{u}_2)}$.
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Special Cases}
\begin{algorithm}[H]
\caption{Sampler when $\B{Q}$ is a diagonal matrix}
\label{algo:multi_diag}
\begin{algorithmic}[1]
\For{$i \in [d]$} \Comment{\textcolor{blue}{In some programming languages, this loop can be vectorized.}} 
\State Draw $\displaystyle{\theta_i \sim \mathcal{N}\pr{\mu_i,1/q_i}}$.
\EndFor\\
\Return $\Bs{\theta} = (\theta_1,\cdots,\theta_d)^{\top}$.
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}
\frametitle{General Cases}
\begin{algorithm}[H]
\caption{Cholesky sampler}
\begin{algorithmic}[1]
\State Set $\B{C} = \text{chol}(\B{Q})$.
\Comment{\textcolor{blue}{$\B{Q} = \B{C}\B{C}^{\top}$}}
\State Draw $\B{z} \sim \mathcal{N}(\B{0}_d,\B{I}_d)$.
\State Solve $\B{C}^{\top}\B{w} = \B{z}$ w.r.t.\ $\B{w}$.\\
\Return $\Bs{\theta} = \Bs{\mu} + \B{w}$.
\end{algorithmic}
\end{algorithm}
Problem: 
\begin{itemize}
\item Computational cost \textcolor{red}{\(\mathcal{O}(d^3 + d^2T)\)} (\(T\) is the number of samples), only when \(\B{Q}\) is unchanged.
\item Storage requirement \textcolor{red}{\(\Theta(d^2)\)}.
\end{itemize}
\end{frame}

\subsection{Methods}
\begin{frame}
\frametitle{More Efficient Solutions}
\begin{itemize}
    \item Square Root approximation: Approximate \(\B{Q}^{1/2}\).
    \item Conjugate Gradient: Solve a linear system w.r.t.\ \(\B{Q}\).
    \item Matrix Splitting: A generalization of Gibbs Sampler.
    \item Data Augmentation: Make use of structure \(\B{Q}= \B{Q}_1 + \B{Q}_2\), introduce auxiliary variable to facilitate sampling.
\end{itemize}
Improvement: 
\begin{itemize}
\item Computational cost \textcolor{red}{$\mathcal{O}(Kd^2T)$} (\(K\) is the number of iterations), or \textcolor{red}{$\mathcal{O}(d^2(T+T_\text{bi}))$} (\(T_\text{bi}\) is the number of burn-in samples).
\item Storage requirement \textcolor{red}{\(\Theta(d)\)}.
\end{itemize}
\end{frame}

\subsection{Guidelines}
\begin{frame}
\frametitle{How to Choose the Sampler}

\begin{tikzpicture}[node distance=0.5cm, align=center]

\node (node0) [ellipse,draw=black] {$\B{Q} = \B{Q}_1 + \B{Q}_2$};

\node (node00) [ellipse,draw=black,below left of=node0, node distance=3.5cm] {Accuracy};
\path [line] (node0) -- node [midway,align=center,left=0.1em] {yes} (node00);

\node (node01) [ellipse,draw=black,below right of=node0, node distance=2.5cm] {$T_{\text{bi}} \ll K T$};
\path [line] (node0) -- node [midway,align=center,right=0.1em] {no} (node01);

\node (node010) [ellipse,draw=black,below left of=node01, node distance=3cm] {Accuracy};
\path [line] (node01) -- node [midway,align=center,left=0.1em] {yes} (node010);

\node (node011) [ellipse,draw=black,below right of=node01, node distance=3cm] {Clustered spectrum};
\path [line] (node01) -- node [midway,align=center,right=0.1em] {no} (node011);

\node (node0111) [rectangle,fill=blue!10,draw=black,below right of=node011, node distance=3cm] {CG};
\path [line] (node011) -- node [midway,align=center,right=0.1em] {no} (node0111);

\node (node0110) [rectangle,fill=blue!10,draw=black,left of=node0111, node distance=1.5cm] {Chebyshev};
\path [line] (node011) -- node [midway,align=center,left=0.1em] {yes} (node0110);

\node (node0101) [rectangle,draw=black,left of=node0110, node distance=2cm] {Approx. \\ MS};
\path [line] (node010) -- node [midway,align=center,right=0.1em] {limited} (node0101);

\node (node0100) [rectangle,fill=blue!10,draw=black,left of=node0101, node distance=2cm] {Chebyshev \\ SSOR};
\path [line] (node010) -- node [midway,align=center,left=0.1em] {arbitrary} (node0100);

\node (node000) [rectangle,draw=black,left of=node0100, node distance=2cm] {(G)EDA};
\path [line] (node00) -- node [midway,align=center,right=0.1em] {} (node000);

\node (node001) [ellipse,draw=black,below left of=node00, node distance=2cm] {$T_{\text{bi}} \ll K T$};
\path [line] (node00) -- node [midway,align=center,left=0.1em] {limited} (node001);

\node (node0011) [rectangle,draw=black,left of=node000, node distance=2cm] {PO};
\path [line] (node001) -- node [midway,align=center,right=0.1em] {no} (node0011);

\node (node0010) [rectangle,draw=black,left of=node0011, node distance=2cm] {Approx. DA};
\path [line] (node001) -- node [midway,align=center,left=0.1em] {yes} (node0010);


\end{tikzpicture}
\end{frame}

\subsection{Example}
\begin{frame}
\frametitle{Bayesian Ridge Regression}
Conditional prior for $\Bs{\theta}$: Gaussian i.i.d.,
\[
\begin{aligned}
p(\Bs{\theta}\mid\tau) &\propto \exp\pr{-\frac{1}{2\tau}||\Bs{\theta}||^2},\\
p(\tau) &\propto \dfrac{1}{\tau}\Bs{1}_{\mathbb{R}_+\setminus\{0\}}(\tau).
\end{aligned}
\]
Posterior: 
\[
  \begin{aligned}
    p(\Bs{\theta},\tau \mid \B{y}) \propto \dfrac{1}{\tau}\Bs{1}_{\mathbb{R}_+\setminus\{0\}}(\tau) \ \exp\Big(-\frac{1}{2\tau}||\Bs{\theta}||^2 - \frac{1}{2\sigma^2}||\B{y}-\B{X}\Bs{\theta}||^2\Big).
  \end{aligned}
\]
\end{frame}

\begin{frame}
\frametitle{Bayesian Ridge Regression (Cont.d)}
Conditional posterior distribution associated to $\Bs{\theta}$: Gaussian with precision matrix and mean vector
\[
  \begin{aligned}
    &\B{Q} = \frac{1}{\sigma^{2}}\B{X}^{\top}\B{X} + \tau^{-1}\B{I}_d, \\
    &\Bs{\mu} = \frac{1}{\sigma^{2}} \B{Q}^{-1}\B{X}^{\top}\B{y}. 
  \end{aligned} 
\]
\begin{figure}
    \centering
    \mbox{{\includegraphics[scale=0.15]{src/images/MNIST_AA.pdf}}}
    \mbox{{\includegraphics[scale=0.15]{src/images/Armstrong_AA.pdf}}}
    \mbox{{\includegraphics[scale=0.15]{src/images/Coepra_AA.pdf}}}
    \caption{Examples of precision matrices $\B{X}^{\top}\B{X}$ for the MNIST, leukemia abd CoEPrA datasets.}
    \label{fig:LASSO_example}
\end{figure}
\end{frame}

\section{Sampling Algorithms From NLA}
\subsection{Polynomial Approximation}
\begin{frame}
\frametitle{Square Root Factorization}
Extension of Cholesky sampler:
\begin{enumerate}
    \item $\B{Q} = \B{U}\B{\Lambda}\B{U^{\top}}$. 
    \item $\B{Q} = \B{B}^2$ with $\B{B} = \B{U}\B{\Lambda}^{1/2}\B{U^{\top}}$.\item $\B{z} \sim \mathcal{N}(\B{0}_d,\B{I}_d)$.
    \item Solve $\B{B}\B{w} = \B{z}$ w.r.t. $\B{w}$ and compute $\Bs{\theta} = \Bs{\mu} + \B{w}$.
\end{enumerate}
We have $f(\B{Q}) = \B{U}f(\B{\Lambda})\B{U}^{\top}$ for real continuous \(f\).

Approximate \(f(\lambda_i) \approx 1/\sqrt{\lambda_i}, \quad \forall i \in [d]\) with Chebyshev polynomials.
\end{frame}

\begin{frame}
\frametitle{Chebyshev Sampler}
The change of interval:
\[g_j = \br{\cos\pr{\pi\frac{2j+1}{2K_{\text{cheby}}}}\dfrac{(\lambda_u-\lambda_l)}{2} + \dfrac{\lambda_u+\lambda_l}{2}}^{-1/2}, \quad j \in [0,K_{\text{cheby}}].\]
The Chebyshev coefficients:
\[c_k = \dfrac{2}{K_{\text{cheby}}} \displaystyle \sum_{j=0}^{K_{\text{cheby}}} g_j \cos\pr{\pi k\frac{2j+1}{2K_{\text{cheby}}}}, \quad k \in [0,K_{\text{cheby}}]. \]
\end{frame}

\begin{frame}
\frametitle{Chebyshev Sampler}
\begin{algorithm}[H]
\caption{Approx.\ square root sampler using Chebyshev polynomials}
\begin{algorithmic}[1] 
\State Draw $\B{z} \sim \mathcal{N}(\B{0}_d,\B{I}_d)$.
\State Set $\alpha = \dfrac{2}{\lambda_u-\lambda_l}$ and $\beta = \dfrac{\lambda_u+\lambda_l}{\lambda_u-\lambda_l}$.
\State Initialize $\B{u}_1 = \alpha\B{Q z} - \beta\B{z}$ and $\B{u}_0 = \B{z}$. 
\State Set $\B{u} = \dfrac{1}{2}c_0\B{u}_0 + c_1\B{u}_1$ and $k=2$.
\While{$k \leq K_{\text{cheby}}$} \Comment{\textcolor{blue}{Compute the $K_{\text{cheby}}$-truncated Chebyshev series.}}
\State Compute $\B{u}' = 2(\alpha\B{Q}\B{u}_1 - \beta\B{u}_1) - \B{u}_0$.
\State Set $\B{u} = \B{u} + c_k\B{u}'$.
\State Set $\B{u}_0 = \B{u}_1$ and $\B{u}_1 = \B{u}'$.
\State $k = k + 1$.
\EndWhile    
\end{algorithmic}
\end{algorithm}
\end{frame}

\subsection{Conjugate Gradient-Based Samplers}
\begin{frame}
\frametitle{Perturbation before Optimization}
Rewrite in \textit{information form}:
\[
  \pi(\Bs{\theta}) \propto \exp\pr{-\dfrac{1}{2}\Bs{\theta}^{\top}\B{Q}\Bs{\theta} + \B{b}^{\top}\Bs{\theta}},
\]
where $\B{b} = \B{Q}\Bs{\mu}$.
\begin{enumerate}
    \item Draw a Gaussian vector $\B{z}' \sim \mathcal{N}(\B{0}_d,\B{Q})$.
    \item Solve a linear system \(\B{Q}\Bs{\theta} = \B{Q}\Bs{\mu} + \B{z}'\) using conjugate gradient methods. (If $\B{u} \sim \mathcal{N}(\B{Q}\Bs{\mu},\B{Q})$, then $\B{Q}^{-1}\B{u} \sim \mathcal{N}(\Bs{\mu},\B{Q}^{-1})$.)
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Optimization with Perturbation}
The linear system we solved
\[\B{Q}\Bs{\theta} = \B{b} + \B{z}'\]
can also be seen as a perturbed version of the linear system
\[\B{Q}\Bs{\theta} = \B{b},\]
where $\B{b} = \B{Q}\Bs{\mu}$.

Add a perturbation step (a univariate Gaussian sampling step) to turn the classical CG solver into a CG sampler.

Sequentially builds a Gaussian vector with a covariance matrix being the best $k$-rank approximation of $\B{Q}^{-1}$ in the Krylov subspace $\mathcal{K}_k(\B{Q},\B{r}^{(0)})$. 
\end{frame}

\begin{frame}
\frametitle{CG Sampler}
\begin{algorithmic}[1]
    \State Set $k=1$, $\B{r}^{(0)} = \B{c} - \B{Q}\Bs{\omega}^{(0)}$, $\B{h}^{(0)} = \B{r}^{(0)}$, $d^{(0)} = \B{h}^{(0)\top}\B{Qh}^{(0)}$ and $\B{y}^{(0)} = \Bs{\omega}^{(0)}$. 
    \While{$\nr{\B{r}^{(k)}} \geq \epsilon$} 
    \State Set $\gamma^{(k-1)} = \dfrac{\B{r}^{(k-1)\top}\B{r}^{(k-1)}}{d^{(k-1)}}$.
    \State Set $\B{r}^{(k)} = \B{r}^{(k-1)} - \gamma^{(k-1)}\B{Qh}^{(k-1)}$.
    \State Set $\eta^{(k)} = -\dfrac{\B{r}^{(k)\top}\B{r}^{(k)}}{\B{r}^{(k-1)T}\B{r}^{(k-1)}}$.
    \State Set $\B{h}^{(k)} = \B{r}^{(k)} - \eta^{(k)}\B{h}^{(k-1)}$.
    \State Set $d^{(k)} = \B{h}^{(k)\top}\B{Qh}^{(k)}$.
    \State Set $\B{y}^{(k)} = \B{y}^{(k-1)} + \dfrac{z}{\sqrt{d^{(k-1)}}}\B{h}^{(k-1)}$ where $z \sim \mathcal{N}(0,1)$. \Comment{\textcolor{blue}{Perturbation}} 
    \State $k = k + 1$.
    \EndWhile
    \State Set $\Bs{\theta} = \Bs{\mu} + \B{y}^{(K_{\text{CG}})}$ where $K_{\text{CG}}$ is the number of CG iterations.\\
\Return $\Bs{\theta}$.
\end{algorithmic}
\end{frame}

\subsection{Summary}
\begin{frame}
\frametitle{Iterative Approaches or Factorization Approaches?}
\begin{itemize}
\item Memory needs of order $\Theta(d^2)$ prohibitive. 
\item If storage not an issue, $K \ll (d+T-1)/T$?
\item Gaussian sampling step embedded within a Gibbs sampler, with a varying covariance or precision matrix: $K \ll d$?
\end{itemize}
\end{frame}

\subsection{Experiments}
\begin{frame}
\frametitle{Settings}
\[
    \B{\Sigma}_{ij} = 2 \exp\pr{-\dfrac{(s_i-s_j)^2}{2a^2}} + \epsilon\delta_{ij}, \quad \forall i,j \in [d]. 
\]
where $\{s_{i}\}_{i\in[d]}$ are evenly spaced scalars on $[-3,3]$, $\epsilon > 0$.

$a = 1.5$ and $\epsilon = 10^{-6}$, small eigenvalues of $\B{\Sigma}$ clustered together near $10^{-6}$.

\begin{figure}
\centering
\mbox{{\includegraphics[scale=0.45]{src/images/simu1_ex1_A.pdf}}}
\mbox{{\includegraphics[scale=0.4]{src/images/simu1_ex1_eigvals.pdf}}}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Results for Accuracy}
\begin{figure}
    \centering
    \mbox{{\includegraphics[scale=0.4]{src/images/simu1_ex1_A_Chol.pdf}}}
    \mbox{{\includegraphics[scale=0.4]{src/images/simu1_ex1_A_CG.pdf}}}
    \mbox{{\includegraphics[scale=0.4]{src/images/simu1_ex1_A_Cheby.pdf}}}
  \caption{Scenario 1. Results of the three considered samplers for the sampling from $\mathcal{N}(\B{0}_d,\B{\Sigma})$ in dimension $d = 100$.}
  \end{figure}
\end{frame}

\begin{frame}
\frametitle{Results for CPU Time}
\begin{figure}
    \centering
    \mbox{{\includegraphics[scale=0.33]{src/images/simu1_ex1_time_1.pdf}}}
    \mbox{{\includegraphics[scale=0.33]{src/images/simu1_ex1_time_100.pdf}}}
    \mbox{{\includegraphics[scale=0.33]{src/images/simu1_ex1_time_1000.pdf}}}
    \mbox{{\includegraphics[scale=0.33]{src/images/simu1_ex1_time_10000.pdf}}}
  \end{figure}
\end{frame}

\begin{frame}
\frametitle{Results When Large Eigenvalues Are Clustered}
\begin{figure}
    \centering
    \mbox{{\includegraphics[scale=0.35]{src/images/simu1_ex1_invA.pdf}}}
    \mbox{{\includegraphics[scale=0.3]{src/images/simu1_ex1_eigvals_invA.pdf}}}
\end{figure}
\begin{figure}
    \centering
    \mbox{{\includegraphics[scale=0.3]{src/images/simu1_ex1_invA_Chol.pdf}}}
    \mbox{{\includegraphics[scale=0.3]{src/images/simu1_ex1_invA_CG.pdf}}}
    \mbox{{\includegraphics[scale=0.3]{src/images/simu1_ex1_invA_Cheby.pdf}}}
  \end{figure}
\end{frame}

\begin{frame}
\frametitle{CG or Chebyshev?}
\begin{figure}
    \centering
    \mbox{{\includegraphics[scale=0.4]{src/images/simu1_ex2_A.pdf}}}
    \mbox{{\includegraphics[scale=0.3]{src/images/simu1_ex2_eigvals_A.pdf}}}
\end{figure}
\begin{figure}
    \centering
    \mbox{{\includegraphics[scale=0.3]{src/images/simu1_ex2_A_Chol.pdf}}}
    \mbox{{\includegraphics[scale=0.3]{src/images/simu1_ex2_A_CG.pdf}}}
    \mbox{{\includegraphics[scale=0.3]{src/images/simu1_ex2_A_Cheby.pdf}}}
  \end{figure}
\end{frame}

\section{Sampling Algorithms From MCMC}
\subsection{Matrix Splitting}
\begin{frame}
\frametitle{Conditional Gaussian Distribution}
If $\Bs{\theta} \sim \mathcal{N}(\Bs{\mu},\B{Q}^{-1})$, then
\[
\begin{aligned}
\text{E}(\Bs{\theta}_i | \Bs{\theta}_{-i}) &= \Bs{\mu} - \frac{1}{\B{Q}_{ii} }\sum_{j\ne i} \B{Q}_{ij} (\Bs{\theta}_j - \Bs{\mu}_j), \\
\text{Prec}(\Bs{\theta}_i | \Bs{\theta}_{-i}) &= \B{Q}_{ii}, \\
\text{Corr}(\Bs{\theta}_i, \Bs{\theta}_j | \Bs{\theta}_{-ij}) &= -\frac{\B{Q}_{ij}}{\sqrt{\B{Q}_{ii}\B{Q}_{jj}}}.
\end{aligned}
\]
Compare the above results with
\[
\begin{aligned}
\text{Var}(\Bs{\theta}_i) &= \Bs{\Sigma}_{ii}, \\
\text{Corr}(\Bs{\theta}_i, \Bs{\theta}_j) &= \frac{\Bs{\Sigma}_{ij}}{\sqrt{\Bs{\Sigma}_{ii}\Bs{\Sigma}_{jj}}}.
\end{aligned}
\]
\end{frame}

\begin{frame}
\frametitle{Gibbs Sampler}
\begin{algorithm}[H]
\caption{Component-wise Gibbs sampler}
\hspace*{\algorithmicindent} \textbf{Input:} Number $T$ of iterations and initialization $\Bs{\theta}^{(0)}$.
\begin{algorithmic}[1]
\State Set $t = 1$.
\While{$t\leq T$}
    \For{$i \in [d]$}
    \State Draw $z \sim \mathcal{N}(0,1)$.
    \State Set $\theta^{(t)}_i = \dfrac{[\B{Q}\Bs{\mu}]_i}{Q_{ii}} + \dfrac{z}{\sqrt{Q_{ii}}} - \dfrac{1}{Q_{ii}}\pr{\displaystyle\sum_{j > i}Q_{ij}\theta_j^{(t-1)} + \displaystyle\sum_{j < i}Q_{ij}\theta_j^{(t)}}$.
    \EndFor
    \State Set $t = t + 1$.
\EndWhile\\
\Return $\Bs{\theta}^{(T)}$.
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}
\frametitle{Rewrite into Gauss--Seidel Linear Systems}
Each iteration solves the linear system
\[(\B{L} + \B{D})\Bs{\theta}^{(t)} = \B{Q}\Bs{\mu} + \B{D}^{1/2}\B{z} - \B{L}^{\top}\Bs{\theta}^{(t-1)},\]
where $\B{z} \sim \mathcal{N}(\B{0}_d,\B{I}_d)$.

By setting $\B{M} = \B{L}+\B{D}$ and $\B{N} = -\B{L}^{\top}$ so that $\B{Q} = \B{M} - \B{N}$,
\[
  \B{M}\Bs{\theta}^{(t)} = \B{Q}\Bs{\mu} +  \tilde{\B{z}}+\B{N}\Bs{\theta}^{(t-1)},
\]
where $\B{N} = -\B{L}^{\top}$ is strictly upper triangular and $\tilde{\B{z}} \sim \mathcal{N}(\B{0}_d,\B{D})$ is easy to sample.
\end{frame}

\begin{frame}
\frametitle{Matrix Splitting Sampler}
\begin{algorithm}[H]
\caption{MCMC sampler based on exact matrix splitting}
\hspace*{\algorithmicindent} \textbf{Input:} Number $T$ of iterations, initialization $\Bs{\theta}^{(0)}$ and splitting $\B{Q} = \B{M} - \B{N}$.
\begin{algorithmic}[1]
\State Set $t=1$.
\While{$t\leq T$}
    \State Draw $\tilde{\B{z}} \sim \mathcal{N}(\B{0}_d,\B{M}^{\top} + \B{N})$.
    \State Solve $\B{M}\Bs{\theta}^{(t)} = \B{Q}\Bs{\mu} + \tilde{\B{z}} + \B{N}\Bs{\theta}^{(t-1)} $ w.r.t. $\Bs{\theta}^{(t)}$.
    \State Set $t = t + 1$.
\EndWhile\\
\Return $\Bs{\theta}^{(T)}$.
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}
\frametitle{Other Matrix Splitting Schemes}
\begin{table}
{\footnotesize
\caption{The matrices $\B{D}$ and $\B{L}$ denote the diagonal and strictly lower triangular parts of $\B{Q}$, respectively, and $\omega$ is a positive scalar.}
\begin{center}
{\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|c|c|} 
    \hline
    \textbf{Sampler} & $\B{M}$ & $\B{N}$ & $\text{cov}(\tilde{\B{z}}) = \B{M}^{\top} + \B{N}$ & convergence\\
    \hline 
    Richardson & $\B{I}_d/\omega$ & $\B{I}_d/\omega - \B{Q}$ & $2\B{I}_d/\omega - \B{Q}$ & $0 < \omega < 2/\nr{\B{Q}}$\\ 
    Jacobi & $\B{D}$ & $\B{D} - \B{Q}$ & $2\B{D} - \B{Q}$ & $|Q_{ii}| > \sum_{j\neq i}|Q_{ij}|$ $\forall i \in [d]$\\
    Gauss--Seidel & $\B{D} + \B{L}$ & $-\B{L}^{\top}$ & $\B{D}$ & always\\
    SOR & $\B{D}/\omega + \B{L}$ & $\frac{1-\omega}{\omega}\B{D} - \B{L}^{\top}$ & $\frac{2-\omega}{\omega}\B{D}$ & $0 < \omega < 2$\\[1em]
    \hline
\end{tabular}}
\end{center}
}
\end{table}
\end{frame}

\begin{frame}
\frametitle{Error of $t$-th Order Polynomial}
Given a linear system $\B{Q}\Bs{\theta} = \B{v}$ and linear solvers based on the matrix splitting $\B{Q} = \B{M}-\B{N}$, consider the recursion,
\[\Bs{\theta}^{(t+1)} = \Bs{\theta}^{(t)} +  \B{M}^{-1}(\B{v}-\B{Q}\Bs{\theta}^{(t)}).\]

The error at iteration $t$,
\[\B{e}^{(t+1)} = \Bs{\theta}^{(t+1)} - \B{Q}^{-1}\B{v},\]
is equal to 
\[\B{e}^{(t+1)} = (\B{I}_d-\B{M}^{-1}\B{Q})^t\B{e}^{(0)}.\]

Can we find another $t$-th order polynomial $\mathsf{P}_t$ that achieves a lower error?
\[\rho(\mathsf{P}_t(\B{M}^{-1}\B{Q})) < \rho((\B{I}_d-\B{M}^{-1}\B{Q})^t).\]
\end{frame}

\begin{frame}
\frametitle{Polynomial Accelerated Solver}
Consider the second-order iterative scheme, for any $t \in \mathbb{N}$, 
$$
\Bs{\theta}^{(t+1)} = \alpha_t\Bs{\theta}^{(t)} + (1-\alpha_t)\Bs{\theta}^{(t-1)} + \beta_t \B{M}^{-1}(\B{v}-\B{Q}\Bs{\theta}^{(t)}),
$$
where $(\alpha_t,\beta_t)_{t \in \mathbb{N}}$ are a set of acceleration parameters.

This iterative method yields an error at step $t$ given by 
\[\B{e}^{(t+1)}=\mathsf{P}_t(\B{M}^{-1}\B{Q})\B{e}^{(0)}, \]
where $\mathsf{P}_t$ stands for a scaled Chebyshev polynomial.

Optimal values for $(\alpha_t,\beta_t)_{t \in \mathbb{N}}$ are given by
$$
\alpha_t = \tau_1\beta_t  \quad \text{and} \quad \beta_t = \pr{\tau_1 - \tau_2^2\beta_{t-1}}^{-1} \ , 
$$
$\tau_1 = [\lambda_{\text{min}}(\B{M}^{-1}\B{Q}) + \lambda_{\text{max}}(\B{M}^{-1}\B{Q})]/2$ and $\tau_2 = [\lambda_{\text{max}}(\B{M}^{-1}\B{Q}) - \lambda_{\text{min}}(\B{M}^{-1}\B{Q})]/4$.
\end{frame}

\begin{frame}
\frametitle{Symmetric Splitting Scheme}
Denote by $\B{M}_{\text{SOR}}$ and $\B{N}_{\text{SOR}}$ the matrices involved in the SOR splitting such that $\B{Q} = \B{M}_{\text{SOR}} - \B{N}_{\text{SOR}}$.

Then for any $0<\omega<2$, the SSOR (symmetric SOR) splitting is defined by $\B{Q} = \B{M}_{\text{SSOR}} - \B{N}_{\text{SSOR}}$ with 
$$
\B{M}_{\text{SSOR}} = \frac{\omega}{2-\omega}\B{M}_{\text{SOR}}\B{D}^{-1}\B{M}_{\text{SOR}}^{\top} \quad \text{and} \quad \B{N}_{\text{SSOR}} = \frac{\omega}{2-\omega}\B{N}_{\text{SOR}}\B{D}^{-1}\B{N}_{\text{SOR}}^{\top} \ .
$$
\end{frame}

\begin{frame}
\frametitle{Approximate Matrix Splitting}
\begin{algorithm}[H]
\caption{MCMC sampler based on approximate matrix splitting}
\hspace*{\algorithmicindent} \textbf{Input:} Number $T$ of iterations, initialization $\Bs{\theta}^{(0)}$ and splitting $\B{Q} = \B{M} - \B{N}$.
\begin{algorithmic}[1]
\State Set $t = 1$.
\While{$t\leq T$}
    \State Draw $\tilde{\B{z}}' \sim \mathcal{N}(\B{0}_d,\tilde{\B{M}})$.   \Comment{ {\textcolor{blue}{$\tilde{\B{M}}=\B{D}$ or $2\pr{\B{D}+ 2\omega\B{I}_d}$.}}}
    \State Solve $\B{M}\Bs{\theta}^{(t)} = \B{Q}\Bs{\mu} + \tilde{\B{z}}' + \B{N}\Bs{\theta}^{(t-1)}$.
    \State Set $t = t + 1$.
\EndWhile\\
\Return $\Bs{\theta}^{(T)}$.
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}
\frametitle{Approximate Matrix Splitting Samplers}
\begin{table}
{
\begin{center}
{\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|c|} 
  \hline
  \textbf{Sampler} & $\B{M}$ & $\B{N}$ & $\mathrm{cov}(\tilde{\B{z}}') = \tilde{\B{M}}$ \\
  \hline 
  Hogwild with blocks of size 1 & $\B{D}$ & $-\B{L} - \B{L}^{\top}$ & $\B{D}$ \\ 
  Clone MCMC & $\B{D} + 2\omega\B{I}_d$ & $2\omega\B{I}_d -\B{L} - \B{L}^{\top}$ & $2\pr{\B{D}+ 2\omega\B{I}_d}$\\[0.3em]
  \hline
\end{tabular}}
\end{center}
}
\end{table}
\[
\widetilde{\B{Q}}_{\mathrm{MS}} =
\begin{cases}
\B{Q}\pr{\B{I}_d - \B{D}^{-1}(\B{L}+\B{L}^{\top})} & \text{for the Hogwild sampler}\\
\B{Q}\pr{\B{I}_d - \frac{1}{2}(\B{D} + 2\omega^{-1}\B{I}_d)^{-1}\B{Q}} & \text{for clone MCMC.}
\end{cases}    \]
\end{frame}

\subsection{Summary}
\begin{frame}
\frametitle{Iterative Sampler or MCMC Sampler?}
\begin{itemize}
\item Iterative Sampler: $K$ iterations to generate one sample. 
\item MCMC Sampler: burn-in period of length $T_{\text{bi}}$.
\item $T + T_{\text{bi}} \ll KT$?
\item MCMC methods when a large number $T\gtrsim T_{\text{bi}}$ of samples is desired. Iterative methods when a small number $T\lesssim T_{\text{bi}}/K$ of samples is desired.
\end{itemize}
\end{frame}

\subsection{Experiments}
\begin{frame}
\end{frame}

\section{Data Augmentation}
\subsection{Exact Data Augmentation}
\begin{frame}
\frametitle{Idea}
\begin{itemize}
\item \(\B{Q} = \B{Q}_1 + \B{Q}_2\) directly from the statistical model under construction.
\item Introduce one (or several) auxiliary variable $\B{u} \in \mathbb{R}^k$. $(\Bs{\theta},\B{u})$ simple conditional distributions, then use Gibbs sampler.
\item Marginalization of $\B{u}$, retrieves the target distribution $\mathcal{N}(\Bs{\mu},\B{Q}^{-1})$: \[\int_{\mathbb{R}^k} \pi(\Bs{\theta},\B{u}) = \pi(\Bs{\theta}).\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{EDA Model}
  Assume that $\B{Q}_2$ presents a particular and simpler structure (e.g., diagonal or circulant) than $\B{Q}_1$. 

  Introduces the joint distribution with p.d.f.
\[
    \pi(\Bs{\theta},\B{u}_1) \propto \exp\pr{-\dfrac{1}{2}\br{(\Bs{\theta}-\Bs{\mu})^{\top}\B{Q}(\Bs{\theta}-\Bs{\mu}) + (\B{u}_1-\Bs{\theta})^{\top}\B{R}(\B{u}_1-\Bs{\theta})}},
\]
  with $\B{R} = \omega^{-1}\B{I}_d - \B{Q}_1$ and $0< \omega < \nr{\B{Q}_1}^{-1}$, where $\nr{\cdot}$ is the spectral norm. 
  
$\B{Q}_1$ and $\B{Q}_2$ decoupled.
\end{frame}

\begin{frame}
  \frametitle{GEDA Model}
When $\B{Q}$ from a hierarchical Bayesian model, $\B{Q}_1 = \B{G}_1^{\top}\Bs{\Lambda}_1\B{G}_1$, where $\Bs{\Lambda}_1$ is a positive definite (and very often diagonal) matrix.
  
GEDA introduces an additional auxiliary variable $\B{u}_2$ such that the augmented p.d.f.\ writes
\[
  \begin{aligned}
    \pi(\Bs{\theta},\B{u}_1,\B{u}_2) &\propto \exp\pr{-\dfrac{1}{2}\br{(\Bs{\theta}-\Bs{\mu})^{\top}\B{Q}(\Bs{\theta}-\Bs{\mu}) + (\B{u}_1-\Bs{\theta})^{\top}\B{R}(\B{u}_1-\Bs{\theta})}}, \\
    &\times \exp\pr{-\dfrac{1}{2}(\B{u}_2-\B{G}_1\B{u}_1)^{\top}\B{\Lambda}_1(\B{u}_2-\B{G}_1\B{u}_1)}.
  \end{aligned}
\]
\end{frame}

\begin{frame}
\frametitle{Algorithm of (G)EDA}
\begin{algorithm}[H]
\caption{Gibbs sampler based on exact data augmentation (G)EDA}
\label{algo:exact_DA}
\hspace*{\algorithmicindent} \textbf{Input:} Number $T$ of iterations and initialization $\Bs{\theta}^{(0)}$, $\B{u}_1^{(0)}$.
\begin{algorithmic}[1]
\State Set $t = 1$.
\While{$t\leq T$}
\State Draw $\B{u}_2^{(t)} \sim \mathcal{N}(\Bs{\mu}_{\B{u}_2},\B{Q}_{\B{u}_2}^{-1})$. \Comment{\textcolor{blue}{Only if GEDA is considered.}}
\State Draw $\B{u}_1^{(t)} \sim \mathcal{N}(\Bs{\mu}_{\B{u}_1},\B{Q}_{\B{u}_1}^{-1})$.
\State Draw $\Bs{\theta}^{(t)} \sim \mathcal{N}(\Bs{\mu}_{\Bs{\theta}},\B{Q}_{\Bs{\theta}}^{-1})$.
\State Set $t = t + 1$.
\EndWhile\\
\Return $\Bs{\theta}^{(T)}$.
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}
\frametitle{Comparison of EDA and GEDA}
\begin{table}
{\scriptsize
\begin{center}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|l|c|c|c|} 
  \hline
  \textbf{Sampler} & $\Bs{\theta} \sim \mathcal{N}(\Bs{\mu}_{\Bs{\theta}},\B{Q}_{\Bs{\theta}}^{-1})$ & $\B{u}_1 \sim \mathcal{N}(\Bs{\mu}_{\B{u}_1},\B{Q}_{\B{u}_1}^{-1})$ & $\B{u}_2  \sim \mathcal{N}(\Bs{\mu}_{\B{u}_2},\B{Q}_{\B{u}_2}^{-1})$ \\
  \hline 
  \multirow{3}{*}{EDA} & $\B{Q}_{\Bs{\theta}} = \omega^{-1}\B{I}_d + \B{Q}_2$ & $\B{Q}_{\B{u}_1} = \B{R}$ & - \\
  & $\Bs{\mu}_{\Bs{\theta}} = \B{Q}_{\Bs{\theta}}^{-1}\pr{\B{R}\B{u}_1 + \B{Q}\Bs{\mu}}$ & $\Bs{\mu}_{\B{u}_1} = \Bs{\theta}$ & - \\[0.3em]
  \hline
  \multirow{3}{*}{GEDA} 
  & $\B{Q}_{\Bs{\theta}} = \omega^{-1}\B{I}_d + \B{Q}_2$ 
  & $\B{Q}_{\B{u}_1} = \omega^{-1}\B{I}_d$ & $\B{Q}_{\B{u}_2} = \B{\Lambda}_1$ \\
  & $\Bs{\mu}_{\Bs{\theta}} = \B{Q}_{\Bs{\theta}}^{-1}(\B{R}\B{u}_1 + \B{Q}\Bs{\mu})$ 
  & $\Bs{\mu}_{\B{u}_1} = \Bs{\theta} - \omega(\B{Q}_1\Bs{\theta} -\B{G}_1^{\top}\B{\Lambda}_1^{-1}\B{u}_2)$ & $\Bs{\mu}_{\B{u}_2} = \B{G}_1\B{u}_1$ \\[0.3em]
  \hline
\end{tabular}}
\end{center}
}
\end{table}
\end{frame}

\section{Unifying Approach}
\subsection{Unifying Proposal Distribution}
\begin{frame}
\frametitle{Surrogate Probability Distribution}
Common form of surrogate p.d.f.:
\[
  \kappa(\Bs{\theta},\B{u}) \propto \pi(\Bs{\theta})\exp\pr{-\frac{1}{2}(\Bs{\theta}-\B{u})^{\top}\B{R}(\Bs{\theta}-\B{u})}\]
  where $\B{u} \in \mathbb{R}^d$ auxiliary variable, $\B{R} \in \mathbb{R}^{d \times d}$ is a symmetric preconditioner.

Restrict $\B{R}$ to be positive definite,
\[  \int_{\mathbb{R}^d} \pi(\Bs{\theta},\B{u}) \mathrm{d}\B{u} = Z^{-1}\pi(\Bs{\theta})\int_{\mathbb{R}^d} \exp\pr{-\frac{1}{2}(\Bs{\theta}-\B{u})^{\top}\B{R}(\Bs{\theta}-\B{u})}\mathrm{d}\B{u} = \pi(\Bs{\theta})\]
holds almost surely with $Z = \mathrm{det}(\B{R})^{-1/2}(2\pi)^{d/2}$.
\end{frame}

\begin{frame}
  \frametitle{From Exact Data Augmentation to Exact Matrix Splitting}
Change of variable $\B{v} = \B{Ru}$,
\[
  \begin{aligned}
    \B{v}\mid\Bs{\theta} &\sim \mathcal{N}\pr{\B{R}\Bs{\theta},\B{R}},\\
    \Bs{\theta}\mid\B{v} &\sim \mathcal{N}\pr{(\B{Q} + \B{R})^{-1}(\B{v}+\B{Q}\Bs{\mu}),(\B{Q} + \B{R})^{-1}}.
  \end{aligned}  
\]
Rewrite the Gibbs sampling steps as an auto-regressive process of order \(1\) w.r.t.\ $\Bs{\theta}$, the sampling strategy is equivalently
\[
\begin{aligned}
  \tilde{\B{z}} &\sim  \mathcal{N}\pr{\B{Q}\Bs{\mu}, 2\B{R} + \B{Q}},\\
  \Bs{\theta}^{(t)} & = \pr{\B{Q}+\B{R}}^{-1}\pr{\tilde{\B{z}}+\B{R}\Bs{\theta}^{(t-1)}}.
\end{aligned} 
\]
\end{frame}

\begin{frame}
  \frametitle{From Exact Data Augmentation to Exact Matrix Splitting (Cont.d)}
  Define $\B{M} = \B{Q}+\B{R}$ and $\B{N} = \B{R}$, 
  \[
    \begin{aligned}
      \tilde{\B{z}} & \sim  \mathcal{N}\pr{\B{Q}\Bs{\mu}, \B{M}^{\top} + \B{N}},
      \Bs{\theta}^{(t)} & =  \B{M}^{-1}\pr{\tilde{\B{z}} + \B{N}\Bs{\theta}^{(t-1)}},
    \end{aligned} 
    \]
  which boils down to the Gibbs sampler based on exact MS.
  
\begin{table}
{\footnotesize
  \caption{Equivalence relations between exact DA and exact MS approaches.} 
  \begin{center}
    {\renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|c|c|c|c|c|} 
      \hline
      $\B{R}=\mathrm{cov}(\B{v}|\Bs{\theta})$ 
      & $(\B{Q} + \B{R})^{-1}=\mathrm{cov}(\Bs{\theta}|\B{v})$ 
      & $\B{M}^{\top} + \B{N}=\mathrm{cov}(\tilde{\B{z}})$ 
      & DA sampler & MS sampler \\
      \hline 
      $\dfrac{\B{I}_d}{\omega} - \B{Q}_1$ 
      & $\pr{\dfrac{\B{I}_d}{\omega} + \B{Q}_2}^{-1}$ 
      & $\dfrac{2\B{I}_d}{\omega} + \B{Q}_2 - \B{Q}_1$ 
      & EDA& Richardson  \\
      $\dfrac{\B{D}_1}{\omega} - \B{Q}_1$ 
      & $\pr{\dfrac{\B{D}_1}{\omega} + \B{Q}_2}^{-1}$ 
      & $\dfrac{2\B{D}_1}{\omega} + \B{Q}_2 - \B{Q}_1$ 
      & {\bf EDAJ} & Jacobi\\[1em]
      \hline
    \end{tabular}}
  \end{center}
}
\end{table}
\end{frame}

\subsection{Gibbs Samplers as Stochastic Versions of PPA}
\begin{frame}
\frametitle{Proximal point algorithm (PPA)}
\begin{algorithm}[H]
\caption{PPA}
\begin{algorithmic}[1]
\State Choose an initial value $\Bs{\theta}^{(0)}$, a positive semi-definite matrix $\B{R}$ and a maximal number of iterations $T$.
\State Set $t=1$.
\While{$t\leq T$}
\State $\Bs{\theta}^{(t)} = \underset{\Bs{\theta}\in\mathbb{R}^d}{\arg \min}\ f(\Bs{\theta}) + \dfrac{1}{2}\nr{\Bs{\theta}-\Bs{\theta}^{(t-1)}}_{\B{R}}^2$.
\EndWhile\\
\Return $\Bs{\theta}^{(T)}$.
\end{algorithmic}
\end{algorithm}
Here \(\nr{\Bs{\theta}}_{\B{R}}^2 \triangleq {\Bs{\theta}^{\top}\B{R}\Bs{\theta}}\) defines the \textit{weighted} norm w.r.t. $\B{R}$ for all $\Bs{\theta} \in \mathbb{R}^d$.

\end{frame}

\begin{frame}
  \frametitle{Revisit Unifying Proposal Distribution}
Recap: \[
  \kappa(\Bs{\theta},\B{u}) \propto \pi(\Bs{\theta})\exp\pr{-\frac{1}{2}(\Bs{\theta}-\B{u})^{\top}\B{R}(\Bs{\theta}-\B{u})}.\]

Define $\B{u} = \Bs{\theta}^{(t-1)}$, then
\[
    \kappa(\Bs{\theta},\B{u}) \triangleq p\pr{\Bs{\theta}|\B{u}=\Bs{\theta}^{(t-1)}} \propto \pi(\Bs{\theta})\exp\pr{-\frac{1}{2}\pr{\Bs{\theta}-\Bs{\theta}^{(t-1)}}^{\top}\B{R}\pr{\Bs{\theta}-\Bs{\theta}^{(t-1)}}}. 
\]

PPA is its deterministic version!
\end{frame}

\begin{frame}
  \frametitle{Connection between Optimization and Simulation}
  
In fact, searching for the maximum a posteriori estimator under the proposal distribution $P(\cdot \mid \Bs{\theta}^{(t-1)})$ with density $p(\cdot \mid \Bs{\theta}^{(t-1)})$ boils down to solving 
\[
    \underset{\Bs{\theta}\in\mathbb{R}^d}{\arg \min}\ \underbrace{-\log\pi(\Bs{\theta})}_{f(\Bs{\theta})} + \dfrac{1}{2}\nr{\Bs{\theta}-\Bs{\theta}^{(t-1)}}_{\B{R}}^2.
\]


\end{frame}
\end{document}